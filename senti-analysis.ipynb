# Import required libraries
!pip install pandas numpy matplotlib seaborn nltk scikit-learn tensorflow wordcloud
# Import all necessery libraries
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
import seaborn as sns
from time import time
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping


# Upload Path to the dataset in Kaggle
file_path = "/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv"
# Load the CSV file into a Pandas DataFrame
df = pd.read_csv(file_path)
# Display the first few rows of the DataFrame
print(f"Dataset loaded: {df.shape[0]} reviews")
print(df.head())

# Sentiment distribution visualization
plt.figure(figsize=(8, 5))
sns.countplot(x='sentiment', data=df)
plt.title('Sentiment Distribution in Dataset')
plt.show()

# Text preprocessing
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # Remove HTML tags
    text = re.sub(r'<[^>]+>', '', text)
    # Remove URLs
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    # Remove special characters and digits
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Convert to lowercase
    text = text.lower()
    # Tokenize and remove stopwords
    words = text.split()
    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]
    return ' '.join(words)
    
print("Preprocessing text...")
start_time = time()
df['cleaned_review'] = df['review'].apply(preprocess_text)
print(f"Preprocessing completed in {time()-start_time:.2f} seconds")
example_review = "I loved the movies"
cleaned_review = preprocess_text(example_review)
print(f"Raw Review: {example_review}")
print(f"Preprocessed Review: {cleaned_review}")

# Train-test split
X = df['cleaned_review']
y = df['sentiment'].map({'positive': 1, 'negative': 0})
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
print(f"Train size: {len(X_train)}, Test size: {len(X_test)}")

# Tokenization and sequencing
max_words = 20000
max_len = 200

print("Tokenizing text...")
tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train)

# Convert text to sequences
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# Pad sequences
X_train_padded = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')
X_test_padded = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')
print(f"Padded sequences shape: {X_train_padded.shape}")

# Build LSTM model
model = Sequential([
    Embedding(input_dim=max_words, output_dim=128, input_length=max_len),
    Bidirectional(LSTM(64, return_sequences=True)),
    Bidirectional(LSTM(32)),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])
model.summary()

model.compile(
    loss='binary_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

# Train with early stopping
early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
print("\nTraining neural network...")
history = model.fit(
    X_train_padded, y_train,
    epochs=15,
    batch_size=128,
    validation_split=0.2,
    callbacks=[early_stop],
    verbose=1
)

# Plot training history
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()

plt.tight_layout()
plt.savefig('training_history.png')
plt.show()

# Evaluate on test set
test_loss, test_acc = model.evaluate(X_test_padded, y_test, verbose=0)
print(f"\nNeural Network Test Accuracy: {test_acc:.4f}")
print(f"Neural Network Test Loss: {test_loss:.4f}")

# Sentiment Analysis Demonstration
def analyze_sentiment(review, model_name="both"):
    """
    Analyze sentiment of a review using our trained models
    """
    print(f"\nReview Text:\n{'-'*50}\n{review}\n{'-'*50}")
    
    # Preprocess the review
    cleaned_review = preprocess_text(review)
    print(f"\nPreprocessed Text:\n{'-'*50}\n{cleaned_review}\n{'-'*50}")
    results = {}
    
    # Neural Network prediction
    if model_name in ["both", "nn"]:
        seq = tokenizer.texts_to_sequences([cleaned_review])
        padded = pad_sequences(seq, maxlen=max_len, padding='post', truncating='post')
        nn_prob = model.predict(padded, verbose=0)[0][0]
        nn_pred = 1 if nn_prob >= 0.5 else 0
        results['neural_network'] = {
            'prediction': 'Positive' if nn_pred == 1 else 'Negative',
            'confidence': nn_prob if nn_pred == 1 else 1 - nn_prob,
            'probabilities': {
                'negative': 1 - nn_prob,
                'positive': nn_prob
            }
        }
    return results

# Sample reviews to demonstrate sentiment analysis
sample_reviews = [
    "This movie was absolutely fantastic! The acting was superb and the storyline was engaging from start to finish. I was completely captivated by the characters and their development throughout the film.",
    "I was extremely disappointed with this movie. The plot was predictable, the acting was wooden, and the special effects looked cheap. I wouldn't recommend it to anyone.",
    "The cinematography in this film was breathtaking, but the script was weak and the pacing was too slow. It had moments of brilliance but overall felt disjointed.",
    "This is without a doubt the worst movie I've ever seen. The dialogue was cringe-worthy, the plot made no sense, and the acting was atrocious. I want my money back!",
    "I loved every minute of this film! The director did an amazing job balancing action and emotion. The soundtrack was perfect and the ending left me wanting more."
]
def visualize_sentiment(results):
    """
    Visualize sentiment prediction results
    """
    if 'neural_network' in results:
        nn_data = results['neural_network']
        plt.bar(['Negative', 'Positive'], 
                [nn_data['probabilities']['negative'], nn_data['probabilities']['positive']],
                color=['red', 'green'])
        plt.ylim(0, 1)
        plt.title('Neural Network Prediction')
        plt.ylabel('Probability')
        plt.text(0, nn_data['probabilities']['negative'] + 0.03, 
                 f"{nn_data['probabilities']['negative']:.2f}", ha='center')
        plt.text(1, nn_data['probabilities']['positive'] + 0.03, 
                 f"{nn_data['probabilities']['positive']:.2f}", ha='center')
        plt.tight_layout()
        plt.show()
# Analyze and visualize each sample review
for i, review in enumerate(sample_reviews):
    print(f"\n\n{'='*60}")
    print(f"ANALYZING REVIEW #{i+1}")
    print(f"{'='*60}")
    
    results = analyze_sentiment(review)
    print("\nSentiment Analysis Results:")
    if 'neural_network' in results:
        nn = results['neural_network']
        print(f"Neural Network: {nn['prediction']} (Confidence: {nn['confidence']:.2%})")
    
    visualize_sentiment(results)

# Function to analyze sentiment for user input review
def analyze_user_review(review):
    cleaned_review = preprocess_text(review)
    print(f"\nPreprocessed Review:\n{cleaned_review}\n")
    
    # Convert input review to sequence and pad it
    seq = tokenizer.texts_to_sequences([cleaned_review])
    padded = pad_sequences(seq, maxlen=max_len, padding='post', truncating='post')
    
    # Predict sentiment using the trained neural network
    nn_prob = model.predict(padded, verbose=0)[0][0]
    nn_pred = 1 if nn_prob >= 0.5 else 0
    sentiment = 'Positive' if nn_pred == 1 else 'Negative'
    confidence = nn_prob if nn_pred == 1 else 1 - nn_prob
    
    print(f"Sentiment: {sentiment} (Confidence: {confidence:.2%})")

# Take user input and analyze sentiment
user_review = input("Enter your movie review: ")
analyze_user_review(user_review)
